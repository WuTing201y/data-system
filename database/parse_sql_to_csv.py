# parse_sql_to_csv.py â€” ç¨ç«‹çš„ SQL dump è§£æå™¨
# åŠŸèƒ½ï¼šå°‡ MySQL dump æª”æ¡ˆè½‰æ›ç‚º CSV æˆ– Excel
# åŸ·è¡Œï¼špython parse_sql_to_csv.py

from pathlib import Path
import pandas as pd
import re
import math
from datetime import datetime

# ==================== è¨­å®š ====================
SQL_PATH = "houseDatabase_version_1.sql"
OUTPUT_CSV = "houses_data.csv"
OUTPUT_EXCEL = "houses_data.xlsx"

DEFAULT_COLS = [
    "trade_date", "year", "quarter", "city", "district", "age_years",
    "area_m2", "area_ping", "price_total", "price_per_ping", "unit_price_m2",
    "usage", "total_floors", "floor", "risk_factor"
]

NEWTAIPEI_29 = [
    "æ¿æ©‹å€","ä¸‰é‡å€","ä¸­å’Œå€","æ°¸å’Œå€","æ–°èŠå€","æ–°åº—å€","æ¨¹æ—å€","é¶¯æ­Œå€","ä¸‰å³½å€",
    "æ·¡æ°´å€","æ±æ­¢å€","ç‘èŠ³å€","åœŸåŸå€","è˜†æ´²å€","äº”è‚¡å€","æ³°å±±å€","æ—å£å€","æ·±å‘å€",
    "çŸ³ç¢‡å€","åªæ—å€","ä¸‰èŠå€","çŸ³é–€å€","å…«é‡Œå€","å¹³æºªå€","é›™æºªå€","è²¢å¯®å€","é‡‘å±±å€",
    "è¬é‡Œå€","çƒä¾†å€"
]

# ==================== è§£æå‡½æ•¸ ====================
def parse_sql_value(val: str):
    """è§£æå–®å€‹ SQL å€¼"""
    val = val.strip()
    if val.upper() == "NULL":
        return None
    if val.startswith("'") and val.endswith("'"):
        return val[1:-1].replace("\\'", "'").replace("\\\\", "\\")
    try:
        if "." in val:
            return float(val)
        return int(val)
    except ValueError:
        return val

def split_tuples(values_text: str) -> list:
    """ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ‡åˆ†ä¸¦è§£æ tuple"""
    values_text = values_text.strip()
    rows = []
    
    # åŒ¹é…æ¯å€‹ (...)
    tuple_pattern = re.compile(r'\(((?:[^()\'"]|\'(?:[^\']|\\.)*\'|"(?:[^"]|\\.)*")*)\)', re.DOTALL)
    
    for match in tuple_pattern.finditer(values_text):
        inner = match.group(1)
        
        # è§£ææ¬„ä½
        fields = []
        current = []
        in_quote = False
        quote_char = None
        escape_next = False
        
        for ch in inner:
            if escape_next:
                current.append(ch)
                escape_next = False
                continue
                
            if ch == '\\':
                current.append(ch)
                escape_next = True
                continue
            
            if ch in ("'", '"') and not in_quote:
                in_quote = True
                quote_char = ch
                current.append(ch)
                continue
            
            if ch == quote_char and in_quote:
                in_quote = False
                quote_char = None
                current.append(ch)
                continue
            
            if ch == ',' and not in_quote:
                fields.append(''.join(current).strip())
                current = []
                continue
            
            current.append(ch)
        
        if current:
            fields.append(''.join(current).strip())
        
        # è§£ææ¯å€‹æ¬„ä½çš„å€¼
        parsed_fields = [parse_sql_value(f) for f in fields]
        
        if len(parsed_fields) == len(DEFAULT_COLS):
            rows.append(parsed_fields)
    
    return rows

def load_sql_to_dataframe(sql_path: str) -> pd.DataFrame:
    """è¼‰å…¥ SQL dump ä¸¦è½‰æ›ç‚º DataFrame"""
    print(f"ğŸ“‚ è®€å–æª”æ¡ˆ: {sql_path}")
    text = Path(sql_path).read_text(encoding="utf-8", errors="ignore")
    
    print("ğŸ” å°‹æ‰¾ INSERT èªå¥...")
    pattern = re.compile(
        r"INSERT\s+INTO\s+`?houses`?\s+VALUES\s*(.+?);",
        flags=re.IGNORECASE | re.DOTALL
    )
    
    all_rows = []
    insert_count = 0
    
    for values_blob in pattern.findall(text):
        insert_count += 1
        rows = split_tuples(values_blob)
        all_rows.extend(rows)
        
        if insert_count % 5 == 0:
            print(f"  è™•ç† {insert_count} å€‹ INSERTï¼Œç´¯è¨ˆ {len(all_rows):,} ç­†è³‡æ–™...")
    
    print(f"âœ… å…±è™•ç† {insert_count} å€‹ INSERT")
    print(f"âœ… è§£æå‡º {len(all_rows):,} ç­†è³‡æ–™")
    
    # å»ºç«‹ DataFrame
    df = pd.DataFrame(all_rows, columns=DEFAULT_COLS)
    
    # è³‡æ–™å‹åˆ¥è½‰æ›
    df["trade_date"] = pd.to_datetime(df["trade_date"], errors="coerce")
    df["year"] = pd.to_numeric(df["year"], errors="coerce")
    df["quarter"] = pd.to_numeric(df["quarter"], errors="coerce")
    df["age_years"] = pd.to_numeric(df["age_years"], errors="coerce")
    df["area_m2"] = pd.to_numeric(df["area_m2"], errors="coerce")
    df["area_ping"] = pd.to_numeric(df["area_ping"], errors="coerce")
    df["price_total"] = pd.to_numeric(df["price_total"], errors="coerce")
    df["price_per_ping"] = pd.to_numeric(df["price_per_ping"], errors="coerce")
    df["unit_price_m2"] = pd.to_numeric(df["unit_price_m2"], errors="coerce")
    df["risk_factor"] = pd.to_numeric(df["risk_factor"], errors="coerce")
    
    return df

# ==================== è³‡æ–™æ¸…ç† ====================
def clean_string(s: str) -> str:
    """æ¸…ç†å­—ä¸²"""
    if s is None:
        return ""
    s = str(s)
    s = re.sub(r"[\uFEFF\u200B-\u200D\u2060]", "", s)  # é›¶å¯¬å­—å…ƒ
    s = re.sub(r"[\x00-\x1F\x7F]", "", s)  # æ§åˆ¶å­—å…ƒ
    s = s.replace("\u3000", "")  # å…¨å½¢ç©ºç™½
    return s.strip()

def normalize_districts(df: pd.DataFrame) -> pd.DataFrame:
    """æ­£è¦åŒ–è¡Œæ”¿å€åç¨±"""
    print("ğŸ”§ æ­£è¦åŒ–è¡Œæ”¿å€åç¨±...")
    
    # æ¸…ç†å­—ä¸²
    for col in ["city", "district", "usage"]:
        if col in df.columns:
            df[col] = df[col].apply(clean_string)
    
    # åŸå¸‚æ­£è¦åŒ–
    def map_city(x: str) -> str:
        if not x:
            return ""
        key = re.sub(r"[\s_-]+", "", x).lower()
        if x in {"æ–°åŒ—å¸‚", "æ–°åŒ—", "æ–°åŒ—ç¸£"}:
            return "æ–°åŒ—å¸‚"
        if key in {"newtaipei", "newtaipeicity", "newtaipecity"}:
            return "æ–°åŒ—å¸‚"
        return x
    
    df["city"] = df["city"].apply(map_city)
    
    # è‹±æ–‡å€åŸŸå°ç…§
    en2zh = {
        "banqiao":"æ¿æ©‹å€", "sanchong":"ä¸‰é‡å€", "zhonghe":"ä¸­å’Œå€", "yonghe":"æ°¸å’Œå€",
        "xinzhuang":"æ–°èŠå€", "xindian":"æ–°åº—å€", "shulin":"æ¨¹æ—å€", "yingge":"é¶¯æ­Œå€",
        "sanxia":"ä¸‰å³½å€", "tamsui":"æ·¡æ°´å€", "danshui":"æ·¡æ°´å€",
        "xizhi":"æ±æ­¢å€", "ruifang":"ç‘èŠ³å€", "tucheng":"åœŸåŸå€", "luzhou":"è˜†æ´²å€",
        "wugu":"äº”è‚¡å€", "taishan":"æ³°å±±å€", "linkou":"æ—å£å€", "shenkeng":"æ·±å‘å€",
        "shiding":"çŸ³ç¢‡å€", "pinglin":"åªæ—å€", "sanzhi":"ä¸‰èŠå€", "shimen":"çŸ³é–€å€",
        "bali":"å…«é‡Œå€", "pingxi":"å¹³æºªå€", "shuangxi":"é›™æºªå€", "gongliao":"è²¢å¯®å€",
        "jinshan":"é‡‘å±±å€", "wanli":"è¬é‡Œå€", "wulai":"çƒä¾†å€"
    }
    
    def norm_dist(x: str) -> str:
        x0 = clean_string(x)
        if not x0:
            return ""
        
        # ä¸­æ–‡å€åŸŸ
        zh = "".join(ch for ch in x0 if "\u4e00" <= ch <= "\u9fff")
        if zh:
            if not zh.endswith("å€"):
                zh = zh + "å€"
            return zh
        
        # è‹±æ–‡å€åŸŸ
        key = re.sub(r"[\s\-_]+", "", x0).lower().replace("district", "")
        return en2zh.get(key, x0)
    
    df["district"] = df["district"].apply(norm_dist)
    df.loc[df["district"].isin(NEWTAIPEI_29), "city"] = "æ–°åŒ—å¸‚"
    
    # ç”¨é€”æ­£è¦åŒ–
    usage_map = {
        "ä½å®…": "ä½å®¶ç”¨",
        "ä½å®¶": "ä½å®¶ç”¨",
        "è¾¦å…¬": "è¾¦å…¬ç”¨",
        "å•†è¾¦": "è¾¦å…¬ç”¨"
    }
    df["usage"] = df["usage"].replace(usage_map)
    
    return df

# ==================== è¨ˆç®—è¡ç”Ÿæ¬„ä½ ====================
def calculate_risk_factor(age):
    """è¨ˆç®—é¢¨éšªä¿‚æ•¸ï¼ˆæ ¹æ“šå±‹é½¡ï¼‰"""
    if pd.isna(age):
        return 0.5
    a0, k = 30.0, 0.12
    return round(1.0 / (1.0 + math.exp(-k * (float(age) - a0))), 3)

def add_derived_columns(df: pd.DataFrame) -> pd.DataFrame:
    """å¢åŠ è¡ç”Ÿæ¬„ä½"""
    print("ğŸ”§ è¨ˆç®—è¡ç”Ÿæ¬„ä½...")
    
    # å¡«è£œé¢¨éšªä¿‚æ•¸
    df["risk_factor"] = df.apply(
        lambda row: calculate_risk_factor(row["age_years"]) if pd.isna(row["risk_factor"]) else row["risk_factor"],
        axis=1
    )
    
    # èª¿æ•´å¾Œæ¯åªå–®åƒ¹
    df["adj_price_per_ping"] = df.apply(
        lambda row: None if pd.isna(row["price_per_ping"]) 
                    else row["price_per_ping"] * (1 - row["risk_factor"]),
        axis=1
    )
    
    # å¹´æœˆæ¬„ä½
    df["year_month"] = df["trade_date"].dt.to_period("M").astype(str)
    
    return df

# ==================== çµ±è¨ˆåˆ†æ ====================
def print_statistics(df: pd.DataFrame):
    """è¼¸å‡ºçµ±è¨ˆè³‡è¨Š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š è³‡æ–™çµ±è¨ˆ")
    print("=" * 80)
    
    print(f"\nç¸½ç­†æ•¸: {len(df):,}")
    print(f"æ—¥æœŸç¯„åœ: {df['trade_date'].min()} ~ {df['trade_date'].max()}")
    
    # åŸå¸‚åˆ†å¸ƒ
    print(f"\nåŸå¸‚åˆ†å¸ƒ:")
    city_counts = df["city"].value_counts()
    for city, count in city_counts.items():
        print(f"  â€¢ {city}: {count:,} ç­†")
    
    # æ–°åŒ—å¸‚å€åŸŸåˆ†å¸ƒ
    newtaipei = df[df["city"] == "æ–°åŒ—å¸‚"]
    if len(newtaipei) > 0:
        print(f"\næ–°åŒ—å¸‚å€åŸŸåˆ†å¸ƒ (å‰ 15):")
        district_counts = newtaipei["district"].value_counts().head(15)
        for district, count in district_counts.items():
            print(f"  â€¢ {district}: {count:,} ç­†")
        
        # æª¢æŸ¥29å€å®Œæ•´æ€§
        found_districts = set(newtaipei["district"].unique())
        missing = [d for d in NEWTAIPEI_29 if d not in found_districts]
        print(f"\nâœ… æ–°åŒ—å¸‚29å€è¦†è“‹: