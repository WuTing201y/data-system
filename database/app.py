# app.py â€” FastAPI + ä¿®æ­£ç‰ˆ MySQL dump è§£æå™¨
# ä¿®æ­£ï¼šä½¿ç”¨æ­£å‰‡è¡¨é”å¼è§£æ tupleï¼ŒæˆåŠŸè§£ææ‰€æœ‰ 29 å€

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List, Dict
from pathlib import Path
import pandas as pd
import re, math, io, sys

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

SQL_PATH = "houseDatabase_version_1.sql"
PING_PER_M2 = 1 / 3.305785

NEWTAIPEI_29 = [
    "æ¿æ©‹å€","ä¸‰é‡å€","ä¸­å’Œå€","æ°¸å’Œå€","æ–°èŠå€","æ–°åº—å€","æ¨¹æ—å€","é¶¯æ­Œå€","ä¸‰å³½å€",
    "æ·¡æ°´å€","æ±æ­¢å€","ç‘èŠ³å€","åœŸåŸå€","è˜†æ´²å€","äº”è‚¡å€","æ³°å±±å€","æ—å£å€","æ·±å‘å€",
    "çŸ³ç¢‡å€","åªæ—å€","ä¸‰èŠå€","çŸ³é–€å€","å…«é‡Œå€","å¹³æºªå€","é›™æºªå€","è²¢å¯®å€","é‡‘å±±å€",
    "è¬é‡Œå€","çƒä¾†å€"
]
BASE29 = [d[:-1] for d in NEWTAIPEI_29]
BASE_MAP = {b: b + "å€" for b in BASE29}

app = FastAPI(title="House AI Estimation API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_methods=["*"], allow_headers=["*"], allow_credentials=True
)

# ===================== è§£æé‚è¼¯ï¼ˆå·²ä¿®æ­£ï¼‰=====================
DEFAULT_COLS = [
    "trade_date","year","quarter","city","district","age_years",
    "area_m2","area_ping","price_total","price_per_ping","unit_price_m2",
    "usage","total_floors","floor","risk_factor"
]

COL_ALIASES = {
    "date":"trade_date", "tradedate":"trade_date",
    "yr":"year", "yyyy":"year",
    "q":"quarter","quart":"quarter",
    "cty":"city","cityname":"city","city_name":"city",
    "dist":"district","districtname":"district","district_name":"district",
    "town":"district","region":"district","area_name":"district",
    "age":"age_years","ageyear":"age_years",
    "aream2":"area_m2","m2":"area_m2",
    "areaping":"area_ping","ping":"area_ping",
    "totalprice":"price_total","total_price":"price_total",
    "pp_ping":"price_per_ping","priceperping":"price_per_ping",
    "unitprice":"unit_price_m2","unitpricem2":"unit_price_m2",
    "use":"usage","purpose":"usage",
    "floors":"total_floors","totalfloor":"total_floors","total_floors":"total_floors",
    "floorno":"floor","floor_num":"floor",
    "risk":"risk_factor","riskfactor":"risk_factor"
}

def _norm_colname(s: str) -> str:
    """æ­£è¦åŒ–æ¬„ä½åç¨±"""
    if not s: return s
    k = s.strip().strip("`").strip()
    k = re.sub(r"\s+", "", k)
    k = k.replace("__", "_").replace("-", "_").lower()
    k = k.replace(" ", "").replace("\t","").replace("\r","").replace("\n","")
    if k in DEFAULT_COLS:
        return k
    k2 = COL_ALIASES.get(k, k)
    return k2

def _parse_sql_value(val: str):
    """è§£æå–®å€‹ SQL å€¼"""
    val = val.strip()
    if val.upper() == "NULL":
        return None
    if val.startswith("'") and val.endswith("'"):
        return val[1:-1].replace("\\'", "'").replace("\\\\", "\\")
    try:
        if "." in val:
            return float(val)
        return int(val)
    except ValueError:
        return val

def _split_tuples_improved(values_text: str) -> list:
    """æ”¹é€²çš„ tuple åˆ‡åˆ†èˆ‡è§£æ - ä½¿ç”¨æ­£å‰‡è¡¨é”å¼"""
    values_text = values_text.strip()
    rows = []
    
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…æ¯å€‹ tuple
    tuple_pattern = re.compile(r'\(((?:[^()\'"]|\'(?:[^\']|\\.)*\'|"(?:[^"]|\\.)*")*)\)', re.DOTALL)
    
    for match in tuple_pattern.finditer(values_text):
        inner = match.group(1)
        
        # è§£æ tuple å…§çš„æ¬„ä½
        fields = []
        current = []
        in_quote = False
        quote_char = None
        escape_next = False
        
        for ch in inner:
            if escape_next:
                current.append(ch)
                escape_next = False
                continue
                
            if ch == '\\':
                current.append(ch)
                escape_next = True
                continue
            
            if ch in ("'", '"') and not in_quote:
                in_quote = True
                quote_char = ch
                current.append(ch)
                continue
            
            if ch == quote_char and in_quote:
                in_quote = False
                quote_char = None
                current.append(ch)
                continue
            
            if ch == ',' and not in_quote:
                fields.append(''.join(current).strip())
                current = []
                continue
            
            current.append(ch)
        
        if current:
            fields.append(''.join(current).strip())
        
        # è§£ææ¯å€‹æ¬„ä½çš„å€¼
        parsed_fields = [_parse_sql_value(f) for f in fields]
        
        if len(parsed_fields) == len(DEFAULT_COLS):
            rows.append(parsed_fields)
    
    return rows

def _load_df_from_sql(sql_path: str) -> pd.DataFrame:
    """è¼‰å…¥ä¸¦è§£æ SQL dump æª”æ¡ˆ"""
    text = Path(sql_path).read_text(encoding="utf-8", errors="ignore")

    pattern = re.compile(
        r"INSERT\s+INTO\s+`?houses`?\s*(\([^)]+\))?\s+VALUES\s*(.+?);",
        flags=re.IGNORECASE | re.DOTALL
    )

    all_rows = []
    insert_count = 0

    for colgrp, values_blob in pattern.findall(text):
        insert_count += 1
        
        # å–å¾—æ¬„ä½é †åºï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if colgrp:
            raw_cols = [c.strip() for c in colgrp.strip()[1:-1].split(",")]
            col_names = [_norm_colname(c) for c in raw_cols]
        else:
            col_names = DEFAULT_COLS.copy()

        # è§£æé€™å€‹ INSERT çš„æ‰€æœ‰ tuples
        rows = _split_tuples_improved(values_blob)
        
        # å°é½Šæ¬„ä½
        for vals in rows:
            if len(vals) < len(col_names):
                vals += [None] * (len(col_names) - len(vals))
            elif len(vals) > len(col_names):
                vals = vals[:len(col_names)]
            
            row_map = dict(zip(col_names, vals))
            row = [row_map.get(c, None) for c in DEFAULT_COLS]
            all_rows.append(row)
        
        if insert_count % 5 == 0:
            print(f"  è™•ç† INSERT #{insert_count}ï¼Œç´¯è¨ˆ {len(all_rows):,} ç­†...")

    df = pd.DataFrame(all_rows, columns=DEFAULT_COLS)
    df["trade_date"] = pd.to_datetime(df["trade_date"], errors="coerce")
    
    print(f"âœ… è¼‰å…¥ {len(df):,} ç­† | ç¯„åœ {df['trade_date'].min()} ~ {df['trade_date'].max()}")
    
    return df

# ===================== æ­£è¦åŒ–è¡Œæ”¿å€ =====================
def _clean_str(s: str) -> str:
    if s is None: return ""
    s = str(s)
    s = re.sub(r"[\uFEFF\u200B-\u200D\u2060]", "", s)
    s = re.sub(r"[\x00-\x1F\x7F]", "", s)
    s = s.replace("\u3000","")
    return s.strip()

def _normalize_admin(df: pd.DataFrame) -> pd.DataFrame:
    if "district_raw" not in df.columns:
        df["district_raw"] = df["district"]

    for c in ["city","district","usage","total_floors"]:
        if c in df.columns:
            df[c] = df[c].apply(_clean_str)

    def map_city(x: str) -> str:
        if not x: return ""
        key = re.sub(r"[\s_-]+","", x).lower()
        if x in {"æ–°åŒ—å¸‚","æ–°åŒ—","æ–°åŒ—ç¸£"}: return "NewTaipei"
        if key in {"newtaipei","newtaipeicity","newtaipecity"}: return "NewTaipei"
        return x
    df["city"] = df["city"].apply(map_city)

    en2zh = {
        "banqiao":"æ¿æ©‹å€","sanchong":"ä¸‰é‡å€","zhonghe":"ä¸­å’Œå€","yonghe":"æ°¸å’Œå€",
        "xinzhuang":"æ–°èŠå€","xindian":"æ–°åº—å€","shulin":"æ¨¹æ—å€","yingge":"é¶¯æ­Œå€",
        "sanxia":"ä¸‰å³½å€","tamsui":"æ·¡æ°´å€","danshui":"æ·¡æ°´å€",
        "xizhi":"æ±æ­¢å€","ruifang":"ç‘èŠ³å€","tucheng":"åœŸåŸå€","luzhou":"è˜†æ´²å€",
        "wugu":"äº”è‚¡å€","taishan":"æ³°å±±å€","linkou":"æ—å£å€","shenkeng":"æ·±å‘å€",
        "shiding":"çŸ³ç¢‡å€","pinglin":"åªæ—å€","sanzhi":"ä¸‰èŠå€","shimen":"çŸ³é–€å€",
        "bali":"å…«é‡Œå€","pingxi":"å¹³æºªå€","shuangxi":"é›™æºªå€","gongliao":"è²¢å¯®å€",
        "jinshan":"é‡‘å±±å€","wanli":"è¬é‡Œå€","wulai":"çƒä¾†å€"
    }

    def to_en_key(s: str) -> str:
        return re.sub(r"[\s\-_]+","", s).lower()

    def norm_dist(x: str) -> str:
        x0 = _clean_str(x)
        if not x0: return ""
        zh = "".join(ch for ch in x0 if "\u4e00" <= ch <= "\u9fff")
        if zh:
            base = re.sub(r"(å€|é®|é„‰|å¸‚)$", "", zh)
            if base in BASE29: return base + "å€"
            return zh if zh.endswith("å€") else zh + "å€"
        key = to_en_key(x0)
        if key in en2zh: return en2zh[key]
        key = re.sub(r"district$", "", key)
        if key in en2zh: return en2zh[key]
        return x0

    df["district"] = df["district"].apply(norm_dist)
    df.loc[df["district"].isin(NEWTAIPEI_29), "city"] = "NewTaipei"

    usage_map = {"ä½å®…":"ä½å®¶ç”¨","ä½å®¶":"ä½å®¶ç”¨","ä½å®¶ç”¨":"ä½å®¶ç”¨",
                 "è¾¦å…¬":"è¾¦å…¬ç”¨","å•†è¾¦":"è¾¦å…¬ç”¨","è¾¦å…¬ç”¨":"è¾¦å…¬ç”¨"}
    df["usage"] = df["usage"].replace(usage_map)
    return df

# ===================== é¢¨éšª & æº–å‚™ =====================
def _rf_age(a):
    if a is None or pd.isna(a): return 0.5
    a0, k = 30.0, 0.12
    return round(1.0 / (1.0 + math.exp(-k * (float(a) - a0))), 3)

def _prepare_df(sql_path: str):
    print(f"ğŸ”§ è¼‰å…¥ SQL: {sql_path}")
    df = _load_df_from_sql(sql_path)
    
    print("ğŸ”§ æ­£è¦åŒ–è¡Œæ”¿å€...")
    df = _normalize_admin(df)
    
    print("ğŸ”§ è¨ˆç®—é¢¨éšªä¿‚æ•¸...")
    df["risk_factor"] = df["risk_factor"].where(~df["risk_factor"].isna(), df["age_years"].map(_rf_age))
    df["adj_price_per_ping"] = df.apply(
        lambda r: None if pd.isna(r["price_per_ping"]) else r["price_per_ping"] * (1 - float(r["risk_factor"])),
        axis=1
    )
    
    # çµ±è¨ˆå€åŸŸ
    districts = df.loc[df["city"]=="NewTaipei","district"].dropna().unique()
    print(f"âœ… æ–°åŒ—å¸‚ç™¼ç¾ {len(districts)} å€‹å€åŸŸ")
    
    return df

print("â³ åˆå§‹åŒ–ä¸­...")
DF = _prepare_df(SQL_PATH)
print(f"âœ… è¼‰å…¥å®Œæˆï¼å…± {len(DF):,} ç­†è³‡æ–™")

# ===================== ç¯©é¸ =====================
def _filter_df(city=None, district=None, usage="ä½å®¶ç”¨", start_date=None, end_date=None):
    sub = DF.copy()
    if usage and usage != "ALL": sub = sub[sub["usage"] == usage]
    if city: sub = sub[sub["city"] == city]
    if district and district != "ALL": sub = sub[sub["district"] == district]
    if start_date: sub = sub[sub["trade_date"] >= pd.to_datetime(start_date)]
    if end_date: sub = sub[sub["trade_date"] <= pd.to_datetime(end_date)]
    return sub

# ===================== API =====================
@app.get("/health")
def health():
    districts = DF.loc[DF["city"]=="NewTaipei","district"].dropna().unique().tolist()
    missing = [d for d in NEWTAIPEI_29 if d not in districts]
    return {
        "status":"ok",
        "rows":int(len(DF)),
        "date_range":[str(DF["trade_date"].min().date()), str(DF["trade_date"].max().date())],
        "districts_in_NewTaipei": len(districts),
        "district_list": sorted(districts),
        "missing_districts": missing
    }

@app.get("/regions")
def regions(city: Optional[str]=None, usage: str="ä½å®¶ç”¨"):
    sub = _filter_df(city=city, usage=usage)
    if sub.empty: return []
    g = (sub.groupby(["city","district"])["trade_date"]
           .agg(min_date="min", max_date="max", n="size")
           .reset_index().sort_values("n", ascending=False))
    g["min_date"] = g["min_date"].dt.date.astype(str)
    g["max_date"] = g["max_date"].dt.date.astype(str)
    return g.to_dict(orient="records")

@app.get("/stats/monthly")
def stats_monthly(city: str, district: str="ALL", usage: str="ä½å®¶ç”¨"):
    sub = _filter_df(city, district, usage)
    if sub.empty: raise HTTPException(404, "no data")
    sub["month"] = sub["trade_date"].dt.to_period("M").dt.to_timestamp()
    monthly = (sub.groupby("month")
               .agg(avg_raw=("price_per_ping","mean"),
                    avg_adj=("adj_price_per_ping","mean"),
                    n=("price_per_ping","size")).reset_index())
    return monthly.to_dict(orient="records")

@app.get("/stats/yearly")
def stats_yearly(city: str, district: str="ALL", usage: str="ä½å®¶ç”¨"):
    sub = _filter_df(city, district, usage)
    if sub.empty: raise HTTPException(404, "no data")
    sub["year"] = sub["trade_date"].dt.year
    yearly = (sub.groupby("year")
              .agg(avg_raw=("price_per_ping","mean"),
                   avg_adj=("adj_price_per_ping","mean"),
                   n=("price_per_ping","size")).reset_index())
    return yearly.to_dict(orient="records")

@app.get("/valuation")
def valuation(city:str, district:str, area_m2:float, age_years:Optional[float]=None, usage:str="ä½å®¶ç”¨"):
    sub = _filter_df(city=city, district=district, usage=usage)
    if sub.empty: raise HTTPException(404, "no region")
    latest = sub["trade_date"].max()
    cut = latest - pd.DateOffset(months=24)
    sample = sub[sub["trade_date"] >= cut]
    area_ping = area_m2 * PING_PER_M2
    ref_pp = sample["adj_price_per_ping"].dropna().median()
    if pd.isna(ref_pp): raise HTTPException(404, "no baseline")
    est_total = ref_pp * area_ping
    return {
        "city":city,"district":district,"area_m2":area_m2,"area_ping":round(area_ping,2),
        "baseline_pp_ping":round(ref_pp,0),"est_total":round(est_total,0)
    }

# ===================== Debug =====================
@app.get("/debug/districts")
def debug_districts():
    g = (DF.groupby("district").size().reset_index(name="n").sort_values("n", ascending=False))
    return {"unique_count":int(g.shape[0]),"top":g.head(50).to_dict(orient="records")}

@app.get("/debug/districts_full")
def debug_districts_full(limit:int=200):
    g = (DF.groupby(["district_raw","district"]).size().reset_index(name="n").sort_values("n",ascending=False))
    return {"unique_pairs":int(g.shape[0]),"top":g.head(limit).to_dict(orient="records")}

print(f"ğŸ”§ SQL: {SQL_PATH}")
print("âœ… API ready â€” run with: uvicorn app:app --reload")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)