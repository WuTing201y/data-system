# app.py â€” FastAPI + Robust MySQL dump parser (no MySQL required)
# Endpoints: /health, /regions, /stats/monthly, /stats/yearly, /valuation

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
from pathlib import Path
import pandas as pd
import re
import math

# ---------- Config ----------
SQL_PATH = "houseDatabase_version_1.sql"   # æ”¾åœ¨å’Œ app.py åŒå±¤
PING_PER_M2 = 1 / 3.305785                  # åªæ•¸æ›ç®—

app = FastAPI(title="House AI Estimation API", version="0.4.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_methods=["*"], allow_headers=["*"], allow_credentials=True
)

# ---------- Low-level SQL parsing ----------
def _split_value_tuples(block: str):
    """
    æŠŠ INSERT ... VALUES å€å¡Šä¸­çš„æ¯ä¸€ç­† "(...)" åˆ‡å‡ºä¾†ã€‚
    å…·å‚™ï¼šè·¨è¡Œã€ä¸­æ–‡ã€è·³è„«å­—å…ƒã€å­—ä¸²å…§é€—è™Ÿä¿è­·ã€‚
    """
    parts, buf, level, in_str, esc = [], [], 0, False, False
    for ch in block:
        if esc:
            buf.append(ch); esc = False; continue
        if ch == "\\":
            buf.append(ch); esc = True; continue
        if ch == "'" and not esc:
            in_str = not in_str
            buf.append(ch); continue
        if not in_str:
            if ch == "(": level += 1
            if ch == ")": level -= 1
        buf.append(ch)
        if level == 0 and ch == ")":
            t = "".join(buf).strip()
            # ä¿éšªï¼šç¢ºä¿å¤–å±¤æœ‰æ‹¬è™Ÿ
            if not t.startswith("("): t = "(" + t
            if not t.endswith(")"): t = t + ")"
            parts.append(t)
            buf = []
    return parts

def _parse_tuple_text(tup: str):
    """
    æŠŠä¸€ç­†å½¢å¦‚ ('2018-09-18',2018,...,NULL) çš„å­—ä¸²è½‰æˆ Python listã€‚
    è¦å‰‡ï¼š
      - '...' -> å­—ä¸²ï¼ˆå«ä¸­æ–‡ï¼‰
      - NULL  -> None
      - å…¶ä»–  -> ç›¡åŠ›è½‰ floatï¼Œä¸è¡Œå°±ä¿ç•™å­—ä¸²
    """
    t = tup.strip()
    if not t.startswith("("): t = "(" + t
    if not t.endswith(")"):  t = t + ")"
    s = t[1:-1]

    fields, tok, in_str, esc, level = [], [], False, False, 0
    for ch in s:
        if esc:
            tok.append(ch); esc = False; continue
        if ch == "\\":
            tok.append(ch); esc = True; continue
        if ch == "'" and level == 0:
            in_str = not in_str
            tok.append(ch); continue
        if not in_str:
            if ch == "(":
                level += 1; tok.append(ch); continue
            if ch == ")":
                level -= 1; tok.append(ch); continue
            if ch == "," and level == 0:
                fields.append("".join(tok).strip()); tok = []; continue
        tok.append(ch)
    if tok:
        fields.append("".join(tok).strip())

    out = []
    for f in fields:
        fu = f.upper()
        if fu == "NULL":
            out.append(None)
        elif len(f) >= 2 and f[0] == "'" and f[-1] == "'":
            val = f[1:-1].replace("\\'", "'").replace("\\\\", "\\")
            out.append(val)
        else:
            try:
                out.append(float(f))
            except Exception:
                out.append(f)
    return out

def _load_df_from_sql(sql_path: str) -> pd.DataFrame:
    text = Path(sql_path).read_text(encoding="utf-8", errors="ignore")

    # æŠ“æ‰€æœ‰ INSERT INTO houses VALUES ... ;
    inserts = re.findall(
        r"INSERT\s+INTO\s+`?houses`?\s+VALUES\s*(.*?);",
        text, flags=re.IGNORECASE | re.DOTALL
    )

    rows, bad = [], 0
    for block in inserts:
        for tup in _split_value_tuples(block):
            try:
                rows.append(_parse_tuple_text(tup))
            except Exception as e:
                bad += 1
                print("âŒ parse fail:", str(e)[:100], "| sample:", tup[:120])

    cols = [
        "trade_date","year","quarter","city","district","age_years",
        "area_m2","area_ping","price_total","price_per_ping","unit_price_m2",
        "usage","total_floors","floor","risk_factor"
    ]
    df = pd.DataFrame(rows, columns=cols)
    df["trade_date"] = pd.to_datetime(df["trade_date"], errors="coerce")
    print(f"âœ… è¼‰å…¥ {len(df):,} ç­†ï¼ˆç•¥é {bad} ç­†ï¼‰| ç¯„åœ {df['trade_date'].min()} ~ {df['trade_date'].max()}")
    return df

# ---------- Feature engineering ----------
def _rf_age(age_years: Optional[float]) -> float:
    """å±‹é½¡é¢¨éšªï¼ˆå¹³æ»‘ logisticï¼Œé¿å…è‡¨ç•Œè·³å‹•ï¼‰"""
    if age_years is None or pd.isna(age_years):
        return 0.5
    a0, k = 30.0, 0.12
    a = float(age_years)
    rf = 1.0 / (1.0 + math.exp(-k * (a - a0)))
    return max(0.03, min(0.98, rf))

def _prepare_df(sql_path: str) -> pd.DataFrame:
    df = _load_df_from_sql(sql_path)
    # è‹¥ dump å…§ risk_factor ç‚ºç©ºï¼Œä»¥å±‹é½¡æ¨ä¼°
    df["risk_factor"] = df["risk_factor"].where(~df["risk_factor"].isna(), df["age_years"].map(_rf_age))
    # æ ¡æ­£å¾Œå–®åƒ¹
    df["adj_price_per_ping"] = df.apply(
        lambda r: None if pd.isna(r["price_per_ping"]) else r["price_per_ping"] * (1 - float(r["risk_factor"])),
        axis=1
    )
    return df

DF = _prepare_df(SQL_PATH)

def _filter_df(city: Optional[str]=None, district: Optional[str]=None,
               usage: str="ä½å®¶ç”¨", start_date: Optional[str]=None,
               end_date: Optional[str]=None) -> pd.DataFrame:
    sub = DF.copy()
    if usage and usage != "ALL":
        sub = sub[sub["usage"] == usage]
    if city:
        sub = sub[sub["city"] == city]
    if district and district != "ALL":
        sub = sub[sub["district"] == district]
    if start_date:
        sub = sub[sub["trade_date"] >= pd.to_datetime(start_date)]
    if end_date:
        sub = sub[sub["trade_date"] <= pd.to_datetime(end_date)]
    return sub

# ---------- Endpoints ----------
@app.get("/health")
def health():
    return {
        "status": "ok",
        "rows": int(len(DF)),
        "date_range": [str(DF["trade_date"].min().date()), str(DF["trade_date"].max().date())]
    }

@app.get("/regions")
def regions(city: Optional[str] = None, usage: str = "ä½å®¶ç”¨"):
    sub = _filter_df(city=city, usage=usage)
    if sub.empty:
        return []
    g = (sub.groupby(["city","district"])["trade_date"]
            .agg(min_date="min", max_date="max", n="size")
            .reset_index().sort_values("n", ascending=False))
    g["min_date"] = g["min_date"].dt.date.astype(str)
    g["max_date"] = g["max_date"].dt.date.astype(str)
    return g.to_dict(orient="records")

@app.get("/stats/monthly")
def stats_monthly(
    city: str,
    district: str = "ALL",
    usage: str = "ä½å®¶ç”¨",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
):
    sub = _filter_df(city, district, usage, start_date, end_date)
    if sub.empty:
        raise HTTPException(404, "no data for given filters")
    sub = sub.copy()
    sub["month"] = sub["trade_date"].dt.to_period("M").dt.to_timestamp()
    monthly = (sub.groupby("month")
                 .agg(avg_raw=("price_per_ping","mean"),
                      avg_adj=("adj_price_per_ping","mean"),
                      n=("price_per_ping","size"))
                 .reset_index())
    return monthly.to_dict(orient="records")

@app.get("/stats/yearly")
def stats_yearly(
    city: str,
    district: str = "ALL",
    usage: str = "ä½å®¶ç”¨",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
):
    sub = _filter_df(city, district, usage, start_date, end_date)
    if sub.empty:
        raise HTTPException(404, "no data for given filters")
    sub = sub.copy()
    sub["year"] = sub["trade_date"].dt.year
    yearly = (sub.groupby("year")
                .agg(avg_raw=("price_per_ping","mean"),
                     avg_adj=("adj_price_per_ping","mean"),
                     n=("price_per_ping","size"))
                .reset_index())
    return yearly.to_dict(orient="records")

@app.get("/valuation")
def valuation(
    city: str, district: str, area_m2: float,
    age_years: Optional[float] = None,
    usage: str = Query("ä½å®¶ç”¨"),
    use_adjusted_baseline: bool = True,
    window_months: int = 24,
    fallback_months: int = 60,
):
    sub = _filter_df(city=city, district=district, usage=usage)
    if sub.empty:
        raise HTTPException(404, f"no region {city}-{district} ({usage})")

    latest = sub["trade_date"].max()
    cut = latest - pd.DateOffset(months=window_months)
    sample = sub[sub["trade_date"] >= cut]
    if sample.empty and fallback_months:
        cut = latest - pd.DateOffset(months=fallback_months)
        sample = sub[sub["trade_date"] >= cut]
    if sample.empty:
        raise HTTPException(404, f"no data between {cut.date()} and {latest.date()}")

    area_ping = area_m2 * PING_PER_M2

    if use_adjusted_baseline:
        ref_pp = sample["adj_price_per_ping"].dropna().median()
        if pd.isna(ref_pp):
            raise HTTPException(404, "no adjusted baseline")
        est_total = ref_pp * area_ping
        baseline_type = "adjusted_median"
        applied_risk = None
    else:
        ref_pp = sample["price_per_ping"].dropna().median()
        if pd.isna(ref_pp):
            raise HTTPException(404, "no raw baseline")
        rf = _rf_age(age_years)
        est_total = ref_pp * (1 - rf) * area_ping
        baseline_type = "raw_median"
        applied_risk = rf

    lo, hi = est_total * 0.9, est_total * 1.1
    return {
        "city": city, "district": district, "usage": usage,
        "area_m2": area_m2, "area_ping": round(area_ping, 2),
        "baseline_type": baseline_type,
        "baseline_pp_ping": None if pd.isna(ref_pp) else round(ref_pp, 0),
        "applied_risk_factor": applied_risk,
        "est_total": round(est_total, 0),
        "est_range_lo": round(lo, 0),
        "est_range_hi": round(hi, 0),
        "cut_from": str(cut.date()), "cut_to": str(latest.date())
    }

print(f"ğŸ”§ SQL: {SQL_PATH}")
print("âœ… API ready â€” run with:  uvicorn app:app --reload")

# å…è¨±: python app.py ç›´æ¥è·‘ï¼ˆä¸ä¸€å®šè¦ç”¨ uvicorn æŒ‡ä»¤ï¼‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)
